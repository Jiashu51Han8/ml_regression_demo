{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning: Regression\n",
    "<b>Problem statement</b>:<br>\n",
    "We are given a collection of samples $\\{(\\mathbf{x}_i,y_i)\\}_{i=1}^N$, where $N$  is the size of the collection, $\\mathbf{x}_i$ is the sample vector containing $D$ features, and $y_i$ is a <b>real-valued</b> target attribute. The goal is to find a mathematical function which given a vector $\\mathbf{x}_k$ of features will output the target value $y_k$.\n",
    "\n",
    "In <b>Linear Regression</b> we assume that this mapping function has the following form:\n",
    "$$f(\\mathbf{x})=\\mathbf{w}\\mathbf{x} + b$$\n",
    "\n",
    "Here $\\mathbf{w}$ is a $D$-dimensional vector of parameters (weights) and $b$ is a constant.<br>\n",
    "The goal is to discover (learn) $f$ from data. Then we can use $f$ to predict $y$ given the feature vector for a new, unknown observation $\\mathbf{x}$: $y \\leftarrow f(\\mathbf{x})$.<br>\n",
    "For example, if $\\mathbf{x}$ has only a single feature (dimension) then we are looking for an equation of the line which fits the relationship between  $\\mathbf{x}$ and $y$ as close as possible:\n",
    "<img src=\"images/linear_regr_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Solution:</b><br>\n",
    "Thus, to find coefficients $\\mathbf{w}$ and $b$ of the best-fitting line, we need to minimize the following expression:\n",
    "\n",
    "$$SSR=\\frac{1}{N} \\Sigma_{i=1}^N(f_{\\mathbf{w},b}(x_i) - y_i)^2$$\n",
    "\n",
    "The expression we are trying to minimize is called an <em>objective function</em>. The expression $(f(x_i) - y_i)^2$ is called the <em>loss function</em>. It is an absolute difference between the value predicted by the model and the empirical observation. \n",
    "\n",
    "In Linear Regression, the loss function is also called the $SSR$ -- the average of the <b>Sum of Squared Residuals</b> computed by the above formula. $SSR$ averages all discrepancies between the model and observed data points.<br>\n",
    "\n",
    "The loss function above is easily differentiable: if we calculate its derivative (gradient) and set it to zero, then we can just solve a system of linear equations to get optimal values of $\\mathbf{w}$ and $b$.\n",
    "(As we know from Calculus: to find the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting home prices\n",
    "Given a vector containing features of a house, we want to be able to predict its price. To build a predictive regression model, we use the dataset ''House Sales in King County, USA'', downloaded from kaggle.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data file [housing.csv](https://docs.google.com/spreadsheets/d/1vk06vuH277_905XORBYQhc4Cf8Poe3BS0AsiBmlzcKA/edit?usp=sharing) to your local directory.<br>\n",
    "__Update the variable `file_name` in the cell below to point to your local directory where you store the datasets for this course__ and then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"../../data_ml_2020/housing.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Explore the data\n",
    "After I did some modifications and preprocessing of the original data, we have the following all-numeric features:\n",
    "<ul>\n",
    "    <li>id - house identifier, numeric.</li>\n",
    "    <li>price - house price, numeric. <b>This is the target variable that we are trying to predict</b>.</li>\n",
    "    <li>bedrooms - no. of bedrooms, numeric.</li>\n",
    "    <li>bathrooms - no. of bathrooms, numeric.</li>\n",
    "    <li>sqft_living - square footage of the home, numeric.</li>\n",
    "    <li>sqft_lot - square footage of the lot, numeric.</li>\n",
    "    <li>floors - no.of floors, numeric.</li>\n",
    "    <li>waterfront - has a view to a waterfront, numeric (0 or 1).</li>\n",
    "    <li>condition - the amount of wear-and-tear, numeric (from 0 to 5).</li>\n",
    "    <li>sqft_above - square footage of house apart from basement, numeric.</li>\n",
    "    <li>sqft_basement - square footage of the basement, numeric.</li>\n",
    "    <li>age - number of years since year built to year sold, numeric.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# this creates a pandas.DataFrame\n",
    "data = pd.read_csv(file_name, index_col='id')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that we have some attributes that have a potential of predicting the price and that we are not wasting our time - let's see if there are any features that are significantly correlated with the price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there any correlation between features?\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "corr = data.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that <em>sqft_living</em> and <em>sqft_above</em> have \n",
    "a positive influence on price, and the house age has the most negative influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['price'] \n",
    "X = data['sqft_living'] \n",
    "\n",
    "# convert to numpy vectors (1D vectors in this case)\n",
    "X=X.values.reshape(len(X),1) \n",
    "Y=Y.values.reshape(len(Y),1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training/testing sets 2:1\n",
    "split_n = len(X)//3 \n",
    "X_train = X[:-split_n] \n",
    "X_test = X[-split_n:] \n",
    "   \n",
    "# Split the targets into training/testing sets \n",
    "Y_train = Y[:-split_n] \n",
    "Y_test = Y[-split_n:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training set \n",
    "regr.fit(X_train, Y_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data points\n",
    "plt.scatter(X_train, Y_train,  color='black') \n",
    "plt.title('Train Data Fit') \n",
    "plt.xlabel('Area, sqft') \n",
    "plt.ylabel('Price') \n",
    "\n",
    "# Plot regression line \n",
    "plt.plot(X_train, regr.predict(X_train), color='red',linewidth=3) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Model evaluation\n",
    "The coefficient of determination, denoted as $R^2$, tells you which amount of variation in $ùë¶$ is explained by the variance in $\\mathbf{ùê±}$, according to our regression model. Larger $R^2$ indicates a better model.<br>\n",
    "The value $R^2=1.0$ corresponds to $SSR = 0$, that is to the perfect fit since the values of predicted and actual responses fit completely with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sq = regr.score(X_train,Y_train)\n",
    "print('intercept:', regr.intercept_)\n",
    "print('slope:', regr.coef_)\n",
    "print(\"Coefficient of determination for train data:\", r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would this model perform on new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sq_test = regr.score(X_test,Y_test)\n",
    "print(\"Coefficient of determination for test data:\", r_sq_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Prediction\n",
    "Now let's take two houses from the dataset and imagine that we want to estimate their prices using our model. The area of the first house is $830$ sqft and the price is \\\\$85,000. The second house with the area of $7300$ sqft and was sold for \\\\$5,300,000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[830], [7390]]\n",
    "y = [85000, 5300000]\n",
    "x, y = np.array(x), np.array(y)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "y_pred = regr.predict(x)\n",
    "print('predicted price:', y_pred)\n",
    "print('actual price', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiple Linear Regression\n",
    "<em>Multiple</em> or Multivariate Linear Regression is used in cases when we have two or more features to predict the target.\n",
    "\n",
    "If there are just two features, the regression function becomes: \n",
    "$$f(x_1, x_2) = b + w_1x_1 + w_2x_2$$ \n",
    "It represents a regression plane in a three-dimensional space.<br> \n",
    "The task becomes to determine the values of the $b$, $w_1$ and $w_2$, such that this plane is as close as possible to the actual responses and yields the minimal $SSR$.<br>\n",
    "In general case with $D>2$ features -- the function has the form:\n",
    "$$f(x_1, \\ldots, x_D) = b + w_1x_1 + \\ldots + w_Dx_D$$\n",
    "and there are $D+1$ parameters to learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Build the model\n",
    "The process is performed as before, but we use all 10 features as input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['price'] \n",
    "X = data[['bedrooms','bathrooms', 'sqft_living','sqft_lot',\n",
    "          'floors', 'waterfront', 'condition', 'sqft_above', \n",
    "          'sqft_basement', 'age']] \n",
    "\n",
    "X=X.values.reshape(len(X),len(X.columns)) \n",
    "Y=Y.values.reshape(len(Y),1) \n",
    "\n",
    "# Split the data into training/testing sets \n",
    "split_n = 2*len(X)//3 \n",
    "X_train = X[:-split_n] \n",
    "X_test = X[-split_n:] \n",
    "   \n",
    "# Split the targets into training/testing sets \n",
    "Y_train = Y[:-split_n] \n",
    "Y_test = Y[-split_n:] \n",
    "\n",
    "regr = linear_model.LinearRegression(normalize=True)\n",
    "regr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Evaluate multivariate model\n",
    "Does this model predict better than the single-variable model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sq_train = regr.score(X_train,Y_train)\n",
    "print(\"Coefficient of determination for train data:\", r_sq_train)\n",
    "print('intercept:', regr.intercept_)\n",
    "print('slope:', regr.coef_)\n",
    "\n",
    "r_sq_test = regr.score(X_test,Y_test)\n",
    "print(\"Coefficient of determination for test data:\", r_sq_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $R^2$ score is significantly better for the train data than for the test data - this is a sign of overfitting: we followed too closely to the training data points and the model failed to generalize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Prediction\n",
    "What about our two houses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[2,1,830,9000,1,0,3,830,0,75], [6,6,7390,24829,2,1,4,5000,2390,24]]\n",
    "y = [85000, 5300000]\n",
    "x, y = np.array(x), np.array(y)\n",
    "\n",
    "y_pred = regr.predict(x)\n",
    "print('predicted price:', y_pred)\n",
    "print('actual price', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polynomial Regression\n",
    "\n",
    "In this case, we assume that the dependence between input and target variables is not linear, but polynomial. \n",
    "\n",
    "In other words, in addition to linear terms like $w_1x_1$, our regression function $f$ can include non-linear terms such as $w_2x_1^2$, $w_3x_1^3$, or even $w_4x_1x_2$, $w_5x_1^2x_2$, and so on.\n",
    "\n",
    "The simplest example of polynomial regression in the case of a single input variable is a polynomial of degree 2: \n",
    "$$f(x) = b + w_1x + w_2x^2$$.\n",
    "\n",
    "Now, we want to compute $b$, $w_1$ and $w_2$.  \n",
    "Keeping this in mind, compare the polynomial regression function with the function $f(x) = b + w_1x_1 + w_2x_2$ used for linear regression. They look very similar and are both linear functions with the unknown coefficients $b$, $w_1$ and $w_2$. This is why we can solve the polynomial regression problem as a linear problem with the term $x^2$ regarded as an additional input variable.\n",
    "\n",
    "In the case of two variables and the polynomial of degree 2, the regression function has this form: \n",
    "\n",
    "$$f([x_1, x_2]) = b + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_2^2  + w_5x_1x_2$$. \n",
    "\n",
    "The algorithm for solving this problem is exactly the same: we apply linear regression learning to 5 input variables: $x_1$, $x_2$, $x_1^2$, $x_2^2$, and $x_1x_2$.  As the result we get the values of six weights which minimize $SSR$: $b$, $w_1$, $w_2$, $w_3$, $w_4$ and $w_5$.\n",
    "\n",
    "### 3.1. Adding derived input variables\n",
    "All we need to do, is to add some more artificial columns to the dataset. We use the <code>PolynomialFeatures</code> of the <code>sklearn.preprocessing</code> package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Polynomial regression with a single variable\n",
    "Let's start with a single input variable -- <code>sqft_living</code>, and the polinomial of degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['price'] \n",
    "X = data['sqft_living'] \n",
    "\n",
    "# convert to numpy vectors (1D vectors in this case)\n",
    "X=X.values.reshape(len(X),1) \n",
    "Y=Y.values.reshape(len(Y),1) \n",
    "   \n",
    "# Split the data into training/testing sets \n",
    "split_n = 2*len(X)//3 \n",
    "X_train = X[:-split_n] \n",
    "X_test = X[-split_n:] \n",
    "   \n",
    "# Split the targets into training/testing sets \n",
    "Y_train = Y[:-split_n] \n",
    "Y_test = Y[-split_n:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate an additional column: $x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "transformer.fit(X_train)\n",
    "X_train_ = transformer.transform(X_train)\n",
    "\n",
    "print(X_train_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset now contains an additional column $x^2$.<br>\n",
    "Everything else is performed exactly as before, but we use an extended <code>$X_train_</code> instead of the original <code>$X_train</code>. Note that we have to do the same transformation for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model as before\n",
    "regr = linear_model.LinearRegression(normalize=True)\n",
    "regr.fit(X_train_, Y_train)    \n",
    "   \n",
    "r_sq_train = regr.score(X_train_,Y_train)\n",
    "print(\"Coefficient of determination for train data:\", r_sq_train)\n",
    "\n",
    "transformer.fit(X_test)\n",
    "X_test_ = transformer.transform(X_test)\n",
    "\n",
    "r_sq_test = regr.score(X_test_,Y_test)\n",
    "print(\"Coefficient of determination for test data:\", r_sq_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Polynomial regression with all the features\n",
    "Let's try to improve the model using polynomial regression for all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['price'] \n",
    "X = data[['bedrooms','bathrooms', 'sqft_living','sqft_lot',\n",
    "          'floors', 'waterfront', 'condition', 'sqft_above', \n",
    "          'sqft_basement', 'age']] \n",
    "\n",
    "X=X.values.reshape(len(X),len(X.columns)) \n",
    "Y=Y.values.reshape(len(Y),1) \n",
    "\n",
    "# Split the data into training/testing sets \n",
    "split_n = 2*len(X)//3 \n",
    "X_train = X[:-split_n] \n",
    "X_test = X[-split_n:] \n",
    "   \n",
    "# Split the targets into training/testing sets \n",
    "Y_train = Y[:-split_n] \n",
    "Y_test = Y[-split_n:] \n",
    "\n",
    "transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "transformer.fit(X_train)\n",
    "X_train_ = transformer.transform(X_train)\n",
    "\n",
    "print(X_train_[0][0], X_train_[0][1], X_train_[0][10], X_train_[0][11] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modified input array contains additional columns: one with the original feature, the other with its square plus the multiplications to the values in all other columns.\n",
    "\n",
    "The model learning algorithm does not change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression(normalize=True)\n",
    "regr.fit(X_train_, Y_train)\n",
    "\n",
    "r_sq_train = regr.score(X_train_,Y_train)\n",
    "print(\"Coefficient of determination for train data:\", r_sq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(X_test)\n",
    "X_test_ = transformer.transform(X_test)\n",
    "\n",
    "r_sq_test = regr.score(X_test_,Y_test)\n",
    "print(\"Coefficient of determination for test data:\", r_sq_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about our two houses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[2,1,830,9000,1,0,3,830,0,75], [6,6,7390,24829,2,1,4,5000,2390,24]]\n",
    "y = [85000, 5300000]\n",
    "x, y = np.array(x), np.array(y)\n",
    "\n",
    "transformer.fit(x)\n",
    "x_ = transformer.transform(x)\n",
    "\n",
    "y_pred = regr.predict(x_)\n",
    "print('predicted price:', y_pred)\n",
    "print('actual price', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the best we can do so far. Fortunately, there are other regression techniques suitable for the cases where linear regression doesn‚Äôt perform well: Support Vector Regression, Decision Trees, Random Forests, and Neural Networks, to name the few.\n",
    "\n",
    "Copyright &copy; 2020 Marina Barsky. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
